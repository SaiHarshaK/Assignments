README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should contain the following files:

 Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [cool root]/src/PA2/lextest.cc
 mycoolc         -> [cool root]/PA2/mycoolc
 stringtab.cc    -> [cool root]/PA2/stringtab.cc
 utilities.cc    -> [cool root]/PA2/utilities.cc
 handle_flags.cc -> [cool root]/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in
[cool root]/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	If you change architectures you must issue

	% make clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

* I define Names for the expressions for better readability and semantics.
* After checking through the cool manual, i have defined the regular expressions for all the required ones.
* Escape sequences are match with the help of '/'
* With the help of '[]', we can match multiple character for that position which is useful to match the regular expressions given keywords are case insensitive.
* '+' is used when we need more than one occurance of that particular expression (defined immediately to the left).
* We use it for matching digits(need atleast one number) and whitespace(to ignore as many whitespace as possible)
* '*' is used when we need one or more than one occurance of that particular expression (defined immediately to the left).
* We use it for matching TYPEID and OBJECTID (the first letter is always small case and not '_')

* Now adter we define the names we can match the input based on rules, and return appropriate tokens.
* For false and true we have to return BOOL_CONST and store the value in cool_yylval.boolean.
* For keywords we return the token corresponding to it.
* For integers, we have to add the integer to inttable and store it in cool_yylval.symbol and return INT_CONST
* For TYPEID/ OBJECTID, we have to add the id to idtable and store it in cool_yylval.symbol and return TYPEID/ OBJECTID

* We return the ascii values of character constants when they are matched.
* On whitespace and we do nothing and go to next input whule on new line we increase the current line number which keeps track of line numbers to show tokens or point out errors.

* We use start conditions with the help of %x and assign name to it so that we can match arbitrary number of rules starting from that start condition.
* We can activate the start condition when needed with the help of "BEGIN".
* Here %x defines a exclusive start condition which means that only rules qualified with that start condition will be active.
* We can call BEGIN(INITIAL) to come out of that start condition state.

* Comments
- For single-line comments, we ignore all characters until a '\n' is met.
- For multi-line comments, when '(*' is parsed, we enter into multi_comment state and increase depth of comments by 1.
- For every new '(*' we increase the depth by 1 and decrease by 1 when met with '*)'.
- For every '\n' we increase curr_lineno and we ignore every other character
- When depth becomes 0 we go back to initial state.
- Error handling
	- When we are in initial state then depth is 0, if we encounter '*)' then the comments are not nested properlyand we return ERROR.
	- If we encounter EOF inside comment we return ERROR.

* String
- When we encounter a '"' we go to "str" state and while setting string-length to 0, null string to false and 0 up the string-buffer
- We handle all escaped sequences in string literal(including '"' and '\') and store them as a character in the buffer.
- When we find escaped newline, we just increase the curr_lineno by 1.
- We handle escaped characters by just ignoring the first '\'.
- As for every other character we add it to the buffer.
- Error handling
	- When we encounter a null character we update the null_string boolean and got to INITIAL state and return ERROR.
	- We do the same when we find a EOF inside the string
	- Finally Based on string does not have null char we add it to the string_table and store it in cool_yylval.symbol

* Finally if there is any character which does not go through all the rules, that charcater is not supported so return error.

* Testcases
- 1
	- All keywords are check with lower and upper alphabets.
- 2
	- Other expressions which are names are checked here
- 3
	- Comments and whitespace are checked here
	- Comments with interleaving -- and (**)
	- Comments with EOF
	- Comments with \n in them
	- White space characters before decalring a object ID
- 4
	- Different Digits are checked here
- 5
	- String literals are checked here
	- String with escaped \n
	- String with no termination "
	- String with escape characters
	- String with \c type, where c are not escape characters
	- String with EOF
- 6
	- Unsupported chars are checked here
	- ERROR token and that string representing them are returned
